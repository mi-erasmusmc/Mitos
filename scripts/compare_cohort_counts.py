#!/usr/bin/env python3

from __future__ import annotations

import argparse
import re
import subprocess
import sys
import textwrap
from pathlib import Path
import time

import ibis
from duckdb import DuckDBPyConnection

try:
    import duckdb
except (
    ModuleNotFoundError
):  # pragma: no cover - optional dependency is validated at runtime.
    duckdb = None

from ibis_cohort.build_context import BuildContext, CohortBuildOptions, compile_codesets
from ibis_cohort.builders.pipeline import build_primary_events
from ibis_cohort.cohort_expression import CohortExpression


def _format_duckdb_setting(value: str) -> str:
    if isinstance(value, str):
        stripped = value.strip()
        if stripped.startswith("'") and stripped.endswith("'"):
            return stripped
        if stripped.startswith('"') and stripped.endswith('"'):
            return stripped
        return f"'{value}'"
    return str(value)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Compare Python/Ibis and Circe-generated cohort row counts for a single JSON."
    )
    parser.add_argument(
        "--json",
        default="cohorts/6243-dementia-outcome-v1.json",
        help="Path to the cohort expression JSON file.",
    )
    parser.add_argument(
        "--cdm-db",
        default="duckyS_local.duckdb",
        help="DuckDB database path that contains the OMOP CDM.",
    )
    parser.add_argument(
        "--cdm-schema",
        default="main",
        help="CDM schema name inside the DuckDB database.",
    )
    parser.add_argument(
        "--vocab-schema",
        help="Vocabulary schema name (defaults to the CDM schema when omitted).",
    )
    parser.add_argument(
        "--result-schema",
        help="Schema that holds the cohort target table (defaults to the CDM schema).",
    )
    parser.add_argument(
        "--target-table",
        default="circe_cohort",
        help="Fully writable cohort table that Circe SQL will insert into.",
    )
    parser.add_argument(
        "--cohort-id",
        type=int,
        default=1,
        help="Cohort definition id used for the Circe target table.",
    )
    parser.add_argument(
        "--temp-schema",
        default="scratch",
        help="Schema used to emulate temporary tables for Circe SQL.",
    )
    parser.add_argument(
        "--python-sql-out",
        help="Optional path to save the Python-generated SQL.",
    )
    parser.add_argument(
        "--circe-sql-out",
        help="Optional path to save the Circe-generated SQL.",
    )
    parser.add_argument(
        "--python-stage-dir",
        help="Directory to save intermediate CREATE TABLE statements generated by the Python pipeline.",
    )
    parser.add_argument(
        "--python-debug-prefix",
        help="Copy each Python stage table into DuckDB as debug_<prefix>_* for post-run inspection.",
    )
    parser.add_argument(
        "--debug-circe",
        action="store_true",
        help="Execute Circe SQL step-by-step and print row counts for key stages.",
    )
    parser.add_argument(
        "--duckdb-memory-limit",
        help="Optional DuckDB memory limit (e.g., 24GB). When provided the Python pipeline connection uses this limit.",
    )
    parser.add_argument(
        "--duckdb-threads",
        type=int,
        help="Optional DuckDB thread count override (e.g., 4).",
    )
    parser.add_argument(
        "--duckdb-preserve-insertion-order",
        choices=["true", "false"],
        help="Set DuckDB preserve_insertion_order flag explicitly.",
    )
    parser.add_argument(
        "--duckdb-temp-dir",
        help="Optional DuckDB temp_directory path for spills.",
    )
    return parser.parse_args()


def quote_ident(value: str) -> str:
    escaped = value.replace('"', '""')
    return f'"{escaped}"'


def qualify_identifier(name: str, schema: str | None) -> str:
    if schema:
        return f"{quote_ident(schema)}.{quote_ident(name)}"
    return quote_ident(name)


def wrap_count_query(sql: str) -> str:
    trimmed = sql.strip().rstrip(";")
    return f"SELECT COUNT(*) AS row_count FROM ({trimmed}) as cohort_rows"


def _split_sql_statements(sql_script: str) -> list[str]:
    statements: list[str] = []
    current: list[str] = []
    in_single = False
    in_double = False
    escape = False
    for ch in sql_script:
        current.append(ch)
        if ch == "\\" and not escape:
            escape = True
            continue
        if ch == "'" and not in_double and not escape:
            in_single = not in_single
        elif ch == '"' and not in_single and not escape:
            in_double = not in_double
        elif ch == ";" and not in_single and not in_double:
            statement = "".join(current).strip()
            if statement:
                statements.append(statement[:-1].strip())
            current = []
        escape = False
    remainder = "".join(current).strip()
    if remainder:
        statements.append(remainder)
    return statements


def debug_circe_steps(
    conn: DuckDBPyConnection,
    sql_script: str,
    duckdb_config: dict[str, str] | None = None,
) -> None:
    statements = _split_sql_statements(sql_script)
    watched_tables = [
        "Codesets",
        "qualified_events",
        "Inclusion_0",
        "inclusion_events",
        "included_events",
        "strategy_ends",
        "cohort_rows",
        "final_cohort",
    ]
    if duckdb_config:
        for key, value in duckdb_config.items():
            conn.execute(f"SET {key}={_format_duckdb_setting(value)}")

    print("\n" + "=" * 80)
    print("CIRCE STEP-BY-STEP ROW COUNTS".center(80))
    print("=" * 80)
    print(f"{'Op':<12} | {'Table':<25} | {'Rows':<12} | Snippet")
    print("-" * 80)

    conn.execute("BEGIN TRANSACTION;")
    try:
        for statement in statements:
            if not statement:
                continue
            snippet = " ".join(statement.split())
            if len(snippet) > 60:
                snippet = snippet[:60] + "..."
            conn.execute(statement)
            upper_stmt = statement.upper()
            op_type = "OTHER"
            target = "-"
            match = None
            if "CREATE TEMP TABLE" in upper_stmt or "CREATE TABLE" in upper_stmt:
                op_type = "CREATE"
                match = re.search(r"TABLE\s+([A-Za-z0-9_\".]+)", statement, re.IGNORECASE)
            elif "INSERT INTO" in upper_stmt:
                op_type = "INSERT"
                match = re.search(r"INSERT\s+INTO\s+([A-Za-z0-9_\".]+)", statement, re.IGNORECASE)
            elif "DELETE FROM" in upper_stmt:
                op_type = "DELETE"
                match = re.search(r"DELETE\s+FROM\s+([A-Za-z0-9_\".]+)", statement, re.IGNORECASE)
            if match:
                target = match.group(1)

            table_name = target.strip('"')
            row_count = "-"
            if op_type != "OTHER" and table_name != "-":
                try_names = [table_name]
                if table_name not in watched_tables:
                    try_names.extend(watched_tables)
                for candidate in try_names:
                    try:
                        count = conn.execute(
                            f'SELECT COUNT(*) FROM "{candidate}"'
                        ).fetchone()[0]
                        row_count = str(count)
                        table_name = candidate
                        break
                    except Exception:
                        continue
            if op_type != "OTHER":
                print(f"{op_type:<12} | {table_name:<25} | {row_count:<12} | {snippet}")
    finally:
        conn.execute("ROLLBACK;")
        print("=" * 80 + "\n")


def run_python_pipeline(
    json_path: Path,
    db_path: str,
    cdm_schema: str,
    vocab_schema: str | None,
    duckdb_config: dict[str, str] | None = None,
    *,
    capture_stages: bool = False,
    debug_prefix: str | None = None,
) -> tuple[str, int, dict[str, float], list[tuple[str, str]]]:
    expression = CohortExpression.model_validate_json(json_path.read_text())
    conn = ibis.duckdb.connect(database=db_path)
    if duckdb_config:
        for key, value in duckdb_config.items():
            conn.raw_sql(f"SET {key}={_format_duckdb_setting(value)}")
    schema = vocab_schema or cdm_schema
    options = CohortBuildOptions(
        cdm_schema=cdm_schema,
        vocabulary_schema=schema,
        capture_sql=capture_stages,
    )
    
    # Phase 1: Compile Codesets (Executes create table in DB)
    compile_start = time.perf_counter()
    resource = compile_codesets(conn, expression.concept_sets, options)
    codeset_exec_ms = (time.perf_counter() - compile_start) * 1000
    
    ctx = BuildContext(conn, options, resource)
    stage_details: list[dict[str, object]] = []
    
    try:
        # Phase 2: Build Events (Constructs Ibis Graph + Materializes Intermediates in DB)
        build_start = time.perf_counter()
        events = build_primary_events(expression, ctx)
        build_exec_ms = (time.perf_counter() - build_start) * 1000
        
        if events is None:
            raise RuntimeError("Cohort expression did not produce any primary events.")
        
        # Phase 3: Compile SQL (Pure String Generation)
        compile_sql_start = time.perf_counter()
        sql = events.compile()
        compile_sql_ms = (time.perf_counter() - compile_sql_start) * 1000
        
        # Phase 4: Final Execution (Executes Final Query in DB)
        count_sql = wrap_count_query(sql)
        count_start = time.perf_counter()
        count = int(conn.raw_sql(count_sql).fetchone()[0])
        final_exec_ms = (time.perf_counter() - count_start) * 1000
        
        if capture_stages:
            for idx, (table_name, statement) in enumerate(ctx.captured_sql(), start=1):
                row_count = int(
                    conn.raw_sql(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
                )
                stage_details.append(
                    {
                        "index": idx,
                        "table": table_name,
                        "row_count": row_count,
                        "sql": statement,
                    }
                )
    finally:
        if capture_stages and debug_prefix and stage_details:
            for stage in stage_details:
                table_name = stage["table"]
                safe_source = table_name.strip('"')
                target_suffix = re.sub(r"[^A-Za-z0-9_]", "_", safe_source)
                target_name = f"{debug_prefix}_{stage['index']:02d}_{target_suffix}"
                conn.raw_sql(
                    f"CREATE OR REPLACE TABLE {quote_ident(target_name)} AS SELECT * FROM {quote_ident(safe_source)}"
                )
        ctx.close()
        if hasattr(conn, "close"):
            conn.close()
            
    metrics = {
        "codeset_exec_ms": codeset_exec_ms,
        "build_exec_ms": build_exec_ms,
        "sql_compile_ms": compile_sql_ms,
        "final_exec_ms": final_exec_ms,
        "total_ms": codeset_exec_ms + build_exec_ms + compile_sql_ms + final_exec_ms,
    }
    return sql, count, metrics, stage_details


def generate_circe_sql_via_r(
    json_path: Path,
    cdm_schema: str,
    vocab_schema: str,
    result_schema: str,
    target_schema: str,
    target_table: str,
    cohort_id: int,
    temp_schema: str,
) -> tuple[str, float]:
    r_script = textwrap.dedent(
        """
        suppressPackageStartupMessages({
          library(CirceR)
          library(SqlRender)
        })
        args <- commandArgs(trailingOnly = TRUE)
        if (length(args) != 8) {
          stop("Expected 8 trailing arguments for JSON and schema metadata.")
        }
        json_path <- args[[1]]
        cdm_schema <- args[[2]]
        vocab_schema <- args[[3]]
        result_schema <- args[[4]]
        target_schema <- args[[5]]
        target_table <- args[[6]]
        cohort_id <- as.integer(args[[7]])
        temp_schema <- args[[8]]

        if (cdm_schema == "") stop("cdm_schema is required")
        if (vocab_schema == "") vocab_schema <- cdm_schema
        if (result_schema == "") result_schema <- cdm_schema
        if (target_schema == "") target_schema <- result_schema
        if (temp_schema == "") temp_schema <- target_schema

        json_str <- paste(readLines(json_path, warn = FALSE), collapse = "\\n")
        expression <- CirceR::cohortExpressionFromJson(json_str)
        options <- CirceR::createGenerateOptions()
        options$generateStats <- FALSE
        options$useTempTables <- FALSE
        options$tempEmulationSchema <- temp_schema
        sql <- CirceR::buildCohortQuery(expression, options)
        sql <- SqlRender::render(
          sql,
          cdm_database_schema = cdm_schema,
          vocabulary_database_schema = vocab_schema,
          results_database_schema = result_schema,
          target_database_schema = target_schema,
          cohort_database_schema = target_schema,
          target_cohort_table = target_table,
          target_cohort_id = cohort_id,
          tempEmulationSchema = temp_schema
        )
        sql <- SqlRender::translate(sql = sql, targetDialect = "duckdb")
        cat(sql)
        """
    ).strip()

    cmd = [
        "Rscript",
        "-e",
        r_script,
        str(json_path),
        cdm_schema or "",
        vocab_schema or "",
        result_schema or "",
        target_schema or "",
        target_table,
        str(cohort_id),
        temp_schema or "",
    ]
    start = time.perf_counter()
    result = subprocess.run(cmd, capture_output=True, text=True)
    elapsed_ms = (time.perf_counter() - start) * 1000
    if result.returncode != 0:
        raise RuntimeError(
            "Circe SQL generation failed:\n"
            f"{result.stderr.strip() or result.stdout.strip()}"
        )
    return result.stdout, elapsed_ms


def execute_circe_sql(
    sql: str,
    db_path: str,
    result_schema: str,
    target_table: str,
    cohort_id: int,
    temp_schema: str,
    duckdb_config: dict[str, str] | None = None,
) -> tuple[int, dict[str, float]]:
    if duckdb is None:
        raise RuntimeError(
            "The 'duckdb' Python package is required to execute Circe SQL. Install it via `pip install duckdb`."
        )
    conn = duckdb.connect(database=db_path, read_only=False)
    if duckdb_config:
        for key, value in duckdb_config.items():
            conn.execute(f"SET {key}={_format_duckdb_setting(value)}")
    qualified_table = qualify_identifier(target_table, result_schema)
    try:
        if result_schema:
            conn.execute(f"CREATE SCHEMA IF NOT EXISTS {quote_ident(result_schema)};")
        if temp_schema:
            conn.execute(f"CREATE SCHEMA IF NOT EXISTS {quote_ident(temp_schema)};")
        conn.execute(
            f"""
            CREATE TABLE IF NOT EXISTS {qualified_table} (
                cohort_definition_id BIGINT,
                subject_id BIGINT,
                cohort_start_date DATE,
                cohort_end_date DATE
            );
            """
        )
        sql_start = time.perf_counter()
        conn.execute(sql)
        sql_exec_ms = (time.perf_counter() - sql_start) * 1000
        count_start = time.perf_counter()
        row_count = conn.execute(
            f"SELECT COUNT(*) FROM {qualified_table} WHERE cohort_definition_id = ?;",
            [cohort_id],
        ).fetchone()[0]
        count_query_ms = (time.perf_counter() - count_start) * 1000
        conn.execute(
            f"DELETE FROM {qualified_table} WHERE cohort_definition_id = ?;",
            [cohort_id],
        )
        metrics = {"sql_exec_ms": sql_exec_ms, "count_query_ms": count_query_ms}
        return int(row_count), metrics
    finally:
        conn.close()


def main() -> int:
    args = parse_args()
    json_path = Path(args.json)
    if not json_path.exists():
        raise SystemExit(f"Cohort JSON not found: {json_path}")

    duckdb_config: dict[str, str] = {}
    if args.duckdb_memory_limit:
        duckdb_config["memory_limit"] = args.duckdb_memory_limit
    if args.duckdb_threads:
        duckdb_config["threads"] = str(args.duckdb_threads)
    if args.duckdb_preserve_insertion_order:
        duckdb_config["preserve_insertion_order"] = args.duckdb_preserve_insertion_order
    if args.duckdb_temp_dir:
        duckdb_config["temp_directory"] = args.duckdb_temp_dir
    if not duckdb_config:
        duckdb_config = None

    current_config = dict(duckdb_config) if duckdb_config else None
    stage_capture = bool(args.python_stage_dir or args.python_debug_prefix)
    python_sql, python_count, python_metrics, python_stages = run_python_pipeline(
        json_path=json_path,
        db_path=args.cdm_db,
        cdm_schema=args.cdm_schema,
        vocab_schema=args.vocab_schema,
        duckdb_config=current_config,
        capture_stages=stage_capture,
        debug_prefix=args.python_debug_prefix,
    )
    if args.python_sql_out:
        Path(args.python_sql_out).write_text(python_sql)
    if args.python_stage_dir and python_stages:
        stage_dir = Path(args.python_stage_dir)
        stage_dir.mkdir(parents=True, exist_ok=True)
        stem = json_path.stem
        for stage in python_stages:
            idx = stage["index"]
            table_name = stage["table"]
            statement = stage["sql"]
            row_count = stage["row_count"]
            safe_table = table_name.replace(".", "_").replace(":", "_")
            stage_path = stage_dir / f"{stem}_stage_{idx:02d}_{safe_table}.sql"
            stage_path.write_text(
                f"{statement};\n-- row_count={row_count}\nSELECT COUNT(*) AS row_count FROM {table_name};\n"
            )

    result_schema = args.result_schema or args.cdm_schema
    target_schema = result_schema

    circe_sql, circe_generate_ms = generate_circe_sql_via_r(
        json_path=json_path,
        cdm_schema=args.cdm_schema,
        vocab_schema=args.vocab_schema or args.cdm_schema,
        result_schema=result_schema,
        target_schema=target_schema,
        target_table=args.target_table,
        cohort_id=args.cohort_id,
        temp_schema=args.temp_schema or result_schema,
    )
    if args.circe_sql_out:
        Path(args.circe_sql_out).write_text(circe_sql)

    circe_config = dict(current_config) if current_config else None
    if args.debug_circe:
        if duckdb is None:
            print("duckdb Python package required for --debug-circe", file=sys.stderr)
        else:
            debug_conn = duckdb.connect(database=args.cdm_db, read_only=False)
            try:
                debug_circe_steps(debug_conn, circe_sql, duckdb_config=circe_config)
            finally:
                debug_conn.close()
    circe_count, circe_exec_metrics = execute_circe_sql(
        sql=circe_sql,
        db_path=args.cdm_db,
        result_schema=target_schema,
        target_table=args.target_table,
        cohort_id=args.cohort_id,
        temp_schema=args.temp_schema or result_schema,
        duckdb_config=circe_config,
    )
    python_total_ms = python_metrics["total_ms"]
    circe_total_ms = circe_generate_ms + circe_exec_metrics.get("sql_exec_ms", 0.0)

    print(
        "Python/Ibis row count: "
        f"{python_count} "
        f"(total_exec={python_total_ms:.1f}ms)"
    )
    print(
        "Circe row count:       "
        f"{circe_count} "
        f"(total_exec={circe_total_ms:.1f}ms)"
    )

    if python_count != circe_count:
        print("Row counts do not match.", file=sys.stderr)
        return 1

    print("Row counts match.")
    return 0


if __name__ == "__main__":
    try:
        raise SystemExit(main())
    except RuntimeError as exc:
        print(str(exc), file=sys.stderr)
        raise SystemExit(1)
